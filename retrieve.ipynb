{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(['https://localhost:9201'], basic_auth=('elastic', 'wfRM-cEYuJW0Cxo0nF_z'), verify_certs=False, ssl_show_warn=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact Checking: retrieve related tables #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optional: you can choose to retrieve tables for all textual claims in tabfact ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def load_all_the_data_tabfact():\n",
    "    all_data = {}\n",
    "    for fnm in [os.path.join(dataset_path, \"collected_data/r1_training_all.json\"), os.path.join(dataset_path, \"collected_data/r2_training_all.json\")]:\n",
    "        infos = eval(open(fnm).read())\n",
    "        all_data.update(infos)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "\n",
    "def get_data_tabfact(which):\n",
    "    # query_text : table_id\n",
    "    all_querys = {}\n",
    "    if which == 'test':\n",
    "        fnm = os.path.join(dataset_path, 'test_id.json')\n",
    "    table_ids = eval(open(fnm).read())\n",
    "    for tab_id in tqdm(table_ids):\n",
    "        tab_data = all_data[tab_id]\n",
    "        qs, labels, caption = tab_data\n",
    "        for i, query in enumerate(qs):\n",
    "            query_text = caption + '. ' + query\n",
    "            # query_text = query\n",
    "            all_querys[query_text] = [tab_id.split('.')[0], labels[i]]\n",
    "\n",
    "    return all_querys\n",
    "\n",
    "\n",
    "dataset_path = './data/tabfact'\n",
    "all_data = load_all_the_data_tabfact()\n",
    "test_claims = get_data_tabfact('test')\n",
    "print(len(test_claims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打开test_claim.json\n",
    "test_claims = json.load(open('./data/tabfact/test_claims.json'))\n",
    "print(len(test_claims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_size = 50\n",
    "recall = 0\n",
    "recall_by_es = 0\n",
    "retrieve_results = {}\n",
    "\n",
    "for query_text, values in test_claims.items():\n",
    "    tab_id = values[0]\n",
    "    # retrieved by elasticsearch\n",
    "    # Search for documents using n-grams\n",
    "    index_name = 'table_index'\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"table\": query_text\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    final_results = es.search(index=index_name, body=query, size=return_size)\n",
    "    retrieve_doc_2 = [ hit['_source']['table id'] for hit in final_results['hits']['hits'] ]\n",
    "    retrieve_results[query_text] = retrieve_doc_2\n",
    "    tab_id = tab_id[:tab_id.index('.')]\n",
    "\n",
    "    if tab_id in retrieve_doc_2:\n",
    "        recall_by_es += 1\n",
    "\n",
    "print(\"recall_rate\")\n",
    "print(recall_by_es / len(test_claims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_size = 5\n",
    "recall_top5 = 0\n",
    "retrieve_query = {}\n",
    "for query_text, values in test_queries.items():\n",
    "    tab_id = values[0]\n",
    "    if query_text in retrieve_results:\n",
    "        if tab_id in retrieve_results[query_text][:return_size]:\n",
    "            recall_top5 += 1\n",
    "        retrieve_query[query_text] = [values[0], values[1], retrieve_results[query_text][:return_size]]\n",
    "\n",
    "print(recall_top5 / len(test_claims))\n",
    "\n",
    "with open('retrieve_tables.json', 'w') as f_index:\n",
    "    json.dump(retrieve_query, f_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Value Imputation: retrieve related tuples and text #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optional: you can choose to retrieve tuples and text for all test tuples ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(['https://localhost:9201'], basic_auth=('elastic', 'wfRM-cEYuJW0Cxo0nF_z'), verify_certs=False, ssl_show_warn=False)\n",
    "\n",
    "return_size = 3\n",
    "retrieved_tuples = {}\n",
    "with open('./data/missing_value_imputation/test.jsonl', 'r') as f:\n",
    "    for tuple_ in f:\n",
    "        tuple_ = json.loads(tuple_)\n",
    "        caption = tuple_['caption']\n",
    "        pgTitle = tuple_['pgTitle']\n",
    "        secTitle = tuple_['secTitle']\n",
    "        headers = tuple_['attributes']\n",
    "        tuple_id = tuple_['tuple id']\n",
    "        values = tuple_['values']\n",
    "        tuple_string = ''\n",
    "\n",
    "        if pgTitle != '':\n",
    "            tuple_string = 'pgTitle: ' + pgTitle + ' |'\n",
    "        if secTitle != '':\n",
    "            tuple_string += ' secTitle: ' + secTitle + ' |' \n",
    "        if caption != '':\n",
    "            tuple_string += ' caption: ' + caption + ' |'\n",
    "\n",
    "\n",
    "        mask_entity = tuple_['mask_entity'] \n",
    "        mask_column = values.index(mask_entity)\n",
    "\n",
    "        for i in range(len(headers)):\n",
    "            if values[i] == '':\n",
    "                continue\n",
    "            if i == mask_column:\n",
    "                tuple_string += ' attribute ' + headers[i] + ' value <mask>'\n",
    "            else:\n",
    "                tuple_string += ' attribute ' + headers[i] + ' value ' + values[i] \n",
    "\n",
    "        retrieved_tuples[tuple_string] = {'retrieved_text': [], 'retrieved_tuples': []}\n",
    "\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"tuple\": tuple_string\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        tuple_results = es.search(index='tuple_index', body=query, size=return_size)\n",
    "\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"text\": tuple_string\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        text_results = es.search(index='text_index', body=query, size=return_size)\n",
    "\n",
    "        for result in tuple_results['hits']['hits']:\n",
    "            retrieved_tuples[tuple_string]['retrieved_tuples'].append(result['_source']['tuple'])\n",
    "        for result in text_results['hits']['hits']:\n",
    "            retrieved_tuples[tuple_string]['retrieved_text'].append(result['_source']['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(['https://localhost:9201'], basic_auth=('elastic', 'wfRM-cEYuJW0Cxo0nF_z'), verify_certs=False, ssl_show_warn=False)\n",
    "\n",
    "return_size = 3\n",
    "retrieved_evidence = {}\n",
    "with open('selected_tuples.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    count = 1\n",
    "    while count < len(lines):\n",
    "        print(\"------------------------------\")\n",
    "        query_text = lines[count].strip()\n",
    "        ground_truth = lines[count+1].strip()\n",
    "        ans = lines[count+2].strip()\n",
    "        query_text = query_text.replace('_',' ')\n",
    "        print(query_text)\n",
    "        print(\"ground_truth: \" + ground_truth)\n",
    "\n",
    "        retrieved_evidence[query_text] = {'retrieved_text': [], 'retrieved_tuples': []}\n",
    "\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"tuple\": query_text\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        tuple_results = es.search(index='tuple_index', body=query, size=return_size)\n",
    "\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"text\": tuple_string\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        text_results = es.search(index='text_index', body=query, size=return_size)\n",
    "\n",
    "        for result in tuple_results['hits']['hits']:\n",
    "            retrieved_evidence[tuple_string]['retrieved_tuples'].append(result['_source']['tuple'])\n",
    "        for result in text_results['hits']['hits']:\n",
    "            retrieved_evidence[tuple_string]['retrieved_text'].append(result['_source']['text'])\n",
    "\n",
    "\n",
    "# 保存retrieved tuples 到json文件中\n",
    "with open('./data/missing_value_imputation/retrieved_evidences.json', 'w') as f:\n",
    "    json.dump(retrieved_evidence, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a6034fbc722b6d7df1bd62739a350770940d75efab2ced85ec2c69a61656d2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
